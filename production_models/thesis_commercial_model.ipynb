{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d29f764",
   "metadata": {},
   "source": [
    "# Langchaing Conversation Model\n",
    "The following code allows us to use conversation models with langchain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffb06c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional packages\n",
    "'''\n",
    "!pip install langchain-openai\n",
    "!pip install openai --updgrade\n",
    "!pip install --force-reinstall -v \"openai==0.28\"\n",
    "!pip install openai==0.28\n",
    "\n",
    "# Useful Resources\n",
    "from langchain_openai import OpenAIEmbeddings # For embbedings\n",
    "from langchain_text_splitters import CharacterTextSplitter # For tokens\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3009f2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://invuniandesai.openai.azure.com/ cf0bd49030ed4aa6a6509be1cd9d604b 2023-05-15 azure gpt-35-turbo-rfmanrique\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Configure needed parameters\n",
    "\n",
    "import os\n",
    "import openai\n",
    "from langchain.chains.conversation.memory import ConversationBufferMemory, ConversationSummaryMemory, ConversationSummaryBufferMemory, ConversationKGMemory, ConversationEntityMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('.env')\n",
    "\n",
    "openai.api_type = \"azure\"\n",
    "\n",
    "\n",
    "# For University Credential\n",
    "\n",
    "openai.api_base = os.getenv(\"OPENAI_ENDPOINT\") \n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai.api_version = \"2023-05-15\"\n",
    "llm_model = 'gpt-35-turbo-rfmanrique'\n",
    "# llm = 'gpt-35-turbo-16k-rfmanrique'\n",
    "# llm_embedding = 'text-embedding-ada-002-rfmanrique'\n",
    "\n",
    "'''\n",
    "# My credentials\n",
    "\n",
    "openai.api_base = os.getenv('AZURE_OPENAI_ENDPOINT')\n",
    "openai.api_key = os.getenv('AZURE_OPENAI_API_KEY')\n",
    "openai.api_version = \"2023-09-15-preview\"\n",
    "llm_model = 'gpt-35-turbo-jdrios'\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "print(openai.api_base, openai.api_key, openai.api_version, openai.api_type, llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a9ef74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define the model\n",
    "llm = ChatOpenAI(temperature = 0, engine= llm_model) # Temperature = 0 means less randomness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b9277c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory Management - Buffer Memory\n",
    "memory = ConversationBufferMemory()\n",
    "conversation_1 = ConversationChain(llm = llm,\n",
    "                                 verbose = False,\n",
    "                                 memory = memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69984164",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Julian! It's nice to meet you. How can I assist you today?\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_1.predict(input=\"Hi there! I'm Julian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464bb290",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836a35af",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_memory = ConversationSummaryMemory(llm=ChatOpenAI(temperature = 0, engine= llm_model))\n",
    "summary_window_memory = ConversationSummaryBufferMemory(llm=ChatOpenAI(temperature = 0, engine= llm_model),\n",
    "                                                        max_token_limit = 40)\n",
    "conversation_2 = ConversationChain(llm = llm,\n",
    "                                 verbose = True,\n",
    "                                 memory = summary_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05f2a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_2.predict(input=\"Hi, I'm Julian, a client of your company\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5fc6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_2.predict(input=\"I'm very angry with your product's quality!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f32af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_2.predict(input=\"Your bicycles are very bad in quality, the chain had break in peaces!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c234e191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the memory buffer for the chat!\n",
    "print(conversation_2.memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd9e20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Knowledge Graph Memory\n",
    "template = \"\"\" The following is a customer service conversation between an human and AI friedly assistant.\n",
    "If the AI doesn't know the answer, say only \"I'm sorry, I don't have this information. Please contact sales.\n",
    "\n",
    "Relevant Information:\n",
    "{history}\n",
    "\n",
    "Conversation:\n",
    "Humna: {input}\n",
    "AI: \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables = [\"history\", \"input\"], template = template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2181cc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_kg = ConversationChain(llm = llm,\n",
    "                                 verbose = True,\n",
    "                                 prompt = prompt,\n",
    "                                 memory = ConversationKGMemory(llm = llm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d356db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entity Memory!!!\n",
    "from langchain.chains.conversation.prompt import ENTITY_MEMORY_CONVERSATION_TEMPLATE\n",
    "conversation_entity = ConversationChain(llm = llm,\n",
    "                                 verbose = True,\n",
    "                                 prompt = ENTITY_MEMORY_CONVERSATION_TEMPLATE,\n",
    "                                 memory = ConversationEntityMemory(llm = llm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4632802e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_entity.predict(input=\"Hi, I'm Julian, a client of your company\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a864a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_entity.predict(input=\"Your bicycles are very bad in quality, the chain had break in peaces!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707a47a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_entity.predict(input=\"I want to know about the warranty, the number is A24124\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742d2a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint(conversation_entity.memory.store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d697cdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "customer_message = \"\"\" Me podrían indicar por favor cual es el horario \\\n",
    "    de atención de la IPS Cuidarte? Necesito sacar una cita \\\n",
    "    con medico general\n",
    "\"\"\"\n",
    "style = \"\"\" Colombian spanish \\\n",
    "    with respectful calm tone\n",
    "\"\"\"\n",
    "prompt = \"\"\" Translate the text \\\n",
    "    into a style that is {style}, \\\n",
    "    text: {text}\n",
    "\"\"\"\n",
    "prompt1 = \"\"\" From the given text, extract the following information: \\\n",
    "    company = The company of the request \\\n",
    "    type = The kind of appoitment \\\n",
    "    \n",
    "    Format the output as JSON with following the next keys:\n",
    "    company\n",
    "    type\n",
    "    text: {text}\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caac28a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: The openai-python library support for Azure OpenAI is in preview.\n",
    "      #Note: This code sample requires OpenAI Python library version 1.0.0 or higher.\n",
    "import os\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('.env')\n",
    "\n",
    "\n",
    "model = AzureChatOpenAI(\n",
    "    azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"), \n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "    api_version=\"2024-02-15-preview\",\n",
    "    azure_deployment=\"gpt-35-turbo-jdrios\"\n",
    ")\n",
    "\n",
    "\n",
    "model('Translate this sentence from English to French. I love programming.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
