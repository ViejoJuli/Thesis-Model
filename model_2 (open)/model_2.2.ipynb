{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e10b60b-94a3-441b-8fe9-2bb542c78995",
   "metadata": {},
   "source": [
    "Chainlit used to create chat interface\n",
    "Link: https://docs.chainlit.io/integrations/langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c319d8",
   "metadata": {},
   "source": [
    "Useful Resources:\n",
    "* Memory: https://www.youtube.com/watch?v=X550Zbz_ROE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5288fa80-2500-46b4-9edb-97a5ec9883d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install huggingface_hub\n",
    "#!pip install transformers\n",
    "#!pip install langchain\n",
    "#!pip install chainlit\n",
    "#!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d1d0a59-e341-4583-8684-4ccb8b44f1f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import openai\n",
    "#import chainlit as cl\n",
    "import os\n",
    "from langchain import HuggingFaceHub, PromptTemplate, LLMChain\n",
    "from langchain.chains import ConversationChain\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f8a57c2-44f7-4d7c-9bc0-b376b37e7d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'InferenceApi' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# model_id = \"gpt2-medium\" # 355M parameter\n",
    "# model_id = \"google/gemma-2b-it\"\n",
    "# model_id = \"google/gemma-7b\"\n",
    "model_id = \"google/flan-ul2\"\n",
    "temperature = 0.1\n",
    "\n",
    "conv_model = HuggingFaceHub(huggingfacehub_api_token=os.environ['HUGGINGFACEHUB_API_TOKEN'],\n",
    "                            repo_id=model_id,\n",
    "                            model_kwargs={\"temperature\": temperature, \"max_new_tokens\":200}) # Temperature is randomness of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ffb1e4b-9e83-4100-98be-8b4d5349e985",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.conversation.memory import ConversationBufferMemory, ConversationSummaryMemory, ConversationSummaryBufferMemory, ConversationKGMemory, ConversationEntityMemory \n",
    "from langchain.chains.conversation.prompt import ENTITY_MEMORY_CONVERSATION_TEMPLATE\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "conv_chain = ConversationChain(llm=conv_model,\n",
    "                               verbose=True,\n",
    "                               prompt=ENTITY_MEMORY_CONVERSATION_TEMPLATE,\n",
    "                               memory=ConversationEntityMemory(llm = conv_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48cd73bc-f17c-4cbb-be4d-d9690d9bc13b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are an assistant to a human, powered by a large language model trained by OpenAI.\n",
      "\n",
      "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
      "\n",
      "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
      "\n",
      "Context:\n",
      "{'Julian.': ''}\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Last line:\n",
      "Human: Hi, I'm Julian. My TV is broken but is under warranty\n",
      "You:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hi, I'm Julian. What can I help you with?\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_chain.predict(input=\"Hi, I'm Julian. My TV is broken but is under warranty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fc511a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are an assistant to a human, powered by a large language model trained by OpenAI.\n",
      "\n",
      "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
      "\n",
      "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
      "\n",
      "Context:\n",
      "{'Julian.': \"Julian's TV is broken but is under warranty.\"}\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, I'm Julian. My TV is broken but is under warranty\n",
      "AI: Hi, I'm Julian. What can I help you with?\n",
      "Last line:\n",
      "Human: How can I get fix? Reference is A1241\n",
      "You:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I can help you with that. What is the reference number?'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_chain.predict(input=\"How can I get fix? Reference is A1241\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb2a48b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are an assistant to a human, powered by a large language model trained by OpenAI.\n",
      "\n",
      "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
      "\n",
      "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
      "\n",
      "Context:\n",
      "{'Julian': '', 'A1241': ''}\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, I'm Julian. My TV is broken but is under warranty\n",
      "AI: Hi, I'm Julian. What can I help you with?\n",
      "Human: How can I get fix? Reference is A1241\n",
      "AI: I can help you with that. What is the reference number?\n",
      "Last line:\n",
      "Human: Reference number is A1241\n",
      "You:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I can help you with that. What is the reference number?'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_chain.predict(input=\"Reference number is A1241\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a8a9bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content=\"Hi, I'm Julian. My TV is broken but is under warranty\", additional_kwargs={}),\n",
      " AIMessage(content=\"Hi, I'm Julian. What can I help you with?\", additional_kwargs={}),\n",
      " HumanMessage(content='How can I get fix? Reference is A1241', additional_kwargs={}),\n",
      " AIMessage(content='I can help you with that. What is the reference number?', additional_kwargs={}),\n",
      " HumanMessage(content='Reference number is A1241', additional_kwargs={}),\n",
      " AIMessage(content='I can help you with that. What is the reference number?', additional_kwargs={})]\n",
      "{'A1241': 'Reference number is A1241',\n",
      " 'Julian': \"Julian's reference number is A1241\",\n",
      " 'Julian.': \"Julian's TV is broken but is under warranty.\"}\n",
      "['Julian', 'A1241']\n"
     ]
    }
   ],
   "source": [
    "# Useful commands\n",
    "from pprint import pprint\n",
    "pprint(conv_chain.memory.buffer)\n",
    "pprint(conv_chain.memory.entity_store.store)\n",
    "pprint(conv_chain.memory.entity_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ef314f",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\" The following is a customer service conversation between an human and AI friedly assistant.\n",
    "If the AI doesn't know the answer, say only \"I'm sorry, I don't have this information. Please contact sales.\n",
    "\n",
    "Relevant Information:\n",
    "{history}\n",
    "\n",
    "Conversation:\n",
    "Humna: {input}\n",
    "AI: \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables = [\"history\", \"input\"], template = template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4d47a7-0948-4c9f-a820-c5cb21dfbbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are a customer service AI chat ready to response to the human user queries\n",
    "{query}\n",
    "\"\"\"\n",
    "user_prompt = PromptTemplate(template=template, input_variables=['query'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
