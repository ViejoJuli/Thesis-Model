{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9bff53e",
   "metadata": {},
   "source": [
    "## Advanced RAG Pipeline\n",
    "* RAGAS: https://www.youtube.com/watch?v=Anr1br0lLz8\n",
    "* Conversation Retrieval Chain: https://js.langchain.com/docs/modules/chains/popular/chat_vector_db\n",
    "* Application: https://abvijaykumar.medium.com/prompt-engineering-retrieval-augmented-generation-rag-cd63cdc6b00\n",
    "* https://medium.com/@jerome.o.diaz/langchain-conversational-retrieval-chain-how-does-it-work-bb2d71cbb665\n",
    "\n",
    "### Steps\n",
    "* Query rephrasing: take in context the memory of the conversation\n",
    "* Relevant document retrieval\n",
    "* Answer rephrasing query with document retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4811e0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('.env')\n",
    "openai.api_base = os.getenv('OPENAI_ENDPOINT')\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "openai.api_version = \"2023-09-15-preview\"\n",
    "llm_model = 'gpt-35-turbo-jdrios'\n",
    "emb_model = 'text-embedding-ada-002-jdrios'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e2859b-596e-40b3-867b-f4d6e91f74bc",
   "metadata": {
    "height": 98,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\n",
    "    [\"https://www.leal.co/usuarios\", \n",
    "     \"https://landing.leal.co/plataforma\",\n",
    "     \"https://www.leal.co/nosotros\"]\n",
    ")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4abc806-64f5-46bb-8c9f-6469ecb18d20",
   "metadata": {
    "height": 64,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Metadata\n",
    "documents[2].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9e1e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 700,\n",
    "    chunk_overlap = 50\n",
    ")\n",
    "\n",
    "documents = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1039a798",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b6a219",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=emb_model,\n",
    "    azure_endpoint=os.getenv('OPENAI_ENDPOINT'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7f2464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Vector Store\n",
    "from langchain_community.vectorstores import FAISS\n",
    "vector_store = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb82cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Retriever\n",
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81604f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to retrieve content\n",
    "retrieved_documents = retriever.invoke(\"Quienes son los fundadores de Leal?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb7a883",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in retrieved_documents:\n",
    "  print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab66ab83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creatin Prompt\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context. If you cannot answer the question with the context, please respond with 'Redirigir...':\n",
    "Context:\n",
    "{context}\n",
    "Question:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "166e5c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "primary_qa_llm = AzureChatOpenAI(model_name=llm_model,\n",
    "                                 temperature=0,\n",
    "                                 api_version=\"2023-09-15-preview\",\n",
    "                                 azure_endpoint=os.getenv('OPENAI_ENDPOINT')) # chain_type is \"stuff\" by default\n",
    "\n",
    "retrieval_augmented_qa_chain = (\n",
    "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
    "    # \"question\" : populated by getting the value of the \"question\" key\n",
    "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
    "    {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
    "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
    "    #              by getting the value of the \"context\" key from the previous step\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
    "    #              into the LLM and stored in a key called \"response\"\n",
    "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
    "    | {\"response\": prompt | primary_qa_llm, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "70ee7245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leal está presente en Colombia, El Salvador, Panamá, Costa Rica, Honduras, Guatemala, Nicaragua y México.\n"
     ]
    }
   ],
   "source": [
    "question = \"En que paises está Leal?\"\n",
    "\n",
    "result = retrieval_augmented_qa_chain.invoke({\"question\" : question})\n",
    "\n",
    "print(result[\"response\"].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8840735d",
   "metadata": {},
   "source": [
    "## Chain Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "21ae8459",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "# Create Memory\n",
    "# memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# Create Chain\n",
    "chatQA = ConversationalRetrievalChain.from_llm(\n",
    "            llm=primary_qa_llm, \n",
    "            retriever=retriever, \n",
    "            # memory=memory,\n",
    "            verbose=True)\n",
    "chat_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "673c072a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "PUNTOS LEAL\n",
      "\n",
      "Acerca de Leal\n",
      "\n",
      "aliadas‍8Países dondetenemos presenciaConoce nuestros fundadoresEstas son las mentes brillantes que están detrás deléxito de nuestro día a día.Camilo MartínezCo-Founder & CEOLinkedIn\n",
      "\n",
      "Florence FrechCo-Founder & COOLinkedIn\n",
      "\n",
      "Conoce nuestras oficinasBogotá, ColombiaEl Salvador, San SalvadorMedellín, ColombiaCiudad de México, México¿Quieres unirte al equipo?HAZLO AQUÍPáginas principalesUsuariosComerciosNosotrosBlog LealPuntosLeal CoinsSeguridadBlog LealSoporteTérminos y condicionesAyuda y contactoTratamiento de datosVencimiento de CoinsActualizar datosCopyright © Leal Colombia | Designed by Leal\n",
      "Human: Como me llamo?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "******************************** dode ********************************\n",
      "Lo siento, pero no tengo acceso a esa información ya que soy un programa de ordenador y no tengo la capacidad de saber tu nombre. ¿Hay algo más en lo que pueda ayudarte?\n"
     ]
    }
   ],
   "source": [
    "query = \"Como me llamo?\"\n",
    "response = chatQA({\"question\": query, \"chat_history\": chat_history})\n",
    "print(\"******************************** Done ********************************\")\n",
    "print(response[\"answer\"])\n",
    "# print(f'Memory buffer: {memory.buffer}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7d8ba01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c456ce1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversation flow\n",
    "chat_history = []\n",
    "qry = \"\"\n",
    "while qry != 'done':\n",
    "    qry = input('Question: ')\n",
    "    if qry != exit:\n",
    "        response = chatQA({\"question\": qry, \"chat_history\": chat_history})\n",
    "        print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b841a0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4bda4f",
   "metadata": {},
   "source": [
    "## Evaluation with RAGAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ff5341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Test Data\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "\n",
    "generator = TestsetGenerator.from_langchain(generator_llm=primary_qa_llm,critic_llm=primary_qa_llm,embeddings=embeddings)\n",
    "testset = generator.generate_with_langchain_docs(documents, test_size=10, \n",
    "                                                 raise_exceptions=False, with_debugging_logs=False,\n",
    "                                                 distributions={simple: 0.5, reasoning: 0.25, multi_context: 0.25})   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55895214",
   "metadata": {},
   "outputs": [],
   "source": [
    "testset.test_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e18f9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = testset.to_pandas()\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5896af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_questions = test_df[\"question\"].values.tolist()\n",
    "test_groundtruths = test_df[\"ground_truth\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0347373",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = []\n",
    "contexts = []\n",
    "\n",
    "for question in test_questions:\n",
    "  response = retrieval_augmented_qa_chain.invoke({\"question\" : question})\n",
    "  answers.append(response[\"response\"].content)\n",
    "  contexts.append([context.page_content for context in response[\"context\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338ad0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "response_dataset = Dataset.from_dict({\n",
    "    \"question\" : test_questions,\n",
    "    \"answer\" : answers,\n",
    "    \"contexts\" : contexts,\n",
    "    \"ground_truth\" : test_groundtruths\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b59900",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27d51c9",
   "metadata": {},
   "source": [
    "## Evaluating process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd56f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    answer_correctness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "\n",
    "metrics = [\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    "    answer_correctness,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed93cfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate(response_dataset, metrics, llm=primary_qa_llm,embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164c0dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = results.to_pandas()\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291132f8",
   "metadata": {},
   "source": [
    "## Better implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147b34b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "retrieval_qa_prompt = hub.pull(\"rlm/rag-prompt\", api_url=\"https://api.hub.langchain.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cfbfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(retrieval_qa_prompt.messages[0].prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9054a3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi Query Retriever\n",
    "from langchain.retrievers import MultiQueryRetriever\n",
    "advanced_retriever = MultiQueryRetriever.from_llm(retriever=retriever, llm=primary_qa_llm)\n",
    "\n",
    "# Document \n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "document_chain = create_stuff_documents_chain(primary_qa_llm, prompt)\n",
    "\n",
    "# Retrieval Chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "retrieval_chain = create_retrieval_chain(advanced_retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e2fa1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = retrieval_chain.invoke({\"input\": \"Que son los Leal Coins\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc61a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4745b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def chatbot_response(user_query, chatbot_name=\"Le A.I\", company_name=\"Leal\"):\n",
    "  \"\"\"\n",
    "  This function simulates a friendly customer service chatbot conversation.\n",
    "\n",
    "  Args:\n",
    "      user_query: The user's question or input.\n",
    "      chatbot_name: The name of the chatbot\n",
    "      company_name: The name of your company\n",
    "\n",
    "  Returns:\n",
    "      A string representing the chatbot's response.\n",
    "  \"\"\"\n",
    "  # Define conversation flow\n",
    "  prompts = [\n",
    "      {\n",
    "          \"START_SEQ\": True,\n",
    "          \"USER_QUERY\": None,\n",
    "          \"RESPONSE A\": f\"Hi there! I'm {chatbot_name}, your friendly customer service assistant for {company_name}. How can I help you today?\",\n",
    "          \"RESPONSE B\": f\"Great to see you! Is there anything I can assist you with on this day?\"\n",
    "      },\n",
    "      {\n",
    "          \"USER_QUERY\": None,\n",
    "          \"RESPONSE A\": f\"I understand you're having trouble with {user_query}. Let's see what we can do to fix that.\",\n",
    "          \"RESPONSE B\": f\"It sounds like you're looking for information about {user_query}. I'm happy to help you find what you need.\"\n",
    "      },\n",
    "      {\n",
    "          \"USER_QUERY\": None,\n",
    "          \"RESPONSE A\": \"Here are a few things you can try: [List your solutions here]. Let me know if any of these work!\",\n",
    "          \"RESPONSE B\": f\"I can definitely walk you through the steps for {user_query}. Would you like me to do that?\",\n",
    "          \"RESPONSE C\": f\"No problem! I've found some helpful resources on {user_query} that you might find useful: [List your resources here].\"\n",
    "      },\n",
    "      {\n",
    "          \"USER_QUERY\": None,\n",
    "          \"RESPONSE A\": \"No worries at all! We'll get this figured out together.\",\n",
    "          \"RESPONSE B\": \"That's a great question! Let me see if I can find an answer for you.\",\n",
    "          \"RESPONSE C\": f\"I apologize for any inconvenience this may have caused. Is there anything else I can do to assist you today?\"\n",
    "      },\n",
    "      {\n",
    "          \"USER_QUERY\": None,\n",
    "          \"RESPONSE A\": \"If the issue seems complex, you can offer to connect the user to a human agent.\",\n",
    "          \"RESPONSE B\": f\"It seems like your situation might require a bit more personalized attention. Would you like me to connect you with one of our customer service representatives?\"\n",
    "      },\n",
    "      {\n",
    "          \"USER_QUERY\": None,\n",
    "          \"RESPONSE A\": \"I hope this information was helpful! Is there anything else I can help you with today?\",\n",
    "          \"RESPONSE B\": \"Glad I could be of assistance! Have a wonderful {day.name}!\"\n",
    "      },\n",
    "      {\n",
    "          \"END_SEQ\": True,\n",
    "          \"USER_QUERY\": None,\n",
    "          \"RESPONSE A\": None,\n",
    "          \"RESPONSE B\": None\n",
    "      }\n",
    "  ]\n",
    "\n",
    "  # Loop through conversation prompts\n",
    "  current_prompt = 0\n",
    "  while current_prompt < len(prompts):\n",
    "    prompt = prompts[current_prompt]\n",
    "    if prompt.get(\"USER_QUERY\") is None or prompt.get(\"USER_QUERY\") == user_query:\n",
    "      response = random.choice([prompt.get(f\"RESPONSE {char}\") for char in \"ABC\" if prompt.get(f\"RESPONSE {char}\")==\"\"])\n",
    "      if response:\n",
    "        print(response)\n",
    "      if prompt.get(\"END_SEQ\"):\n",
    "        break\n",
    "    current_prompt += 1\n",
    "\n",
    "# Example usage\n",
    "chatbot_response(\"I'm having trouble logging in.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
