{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d29f764",
   "metadata": {},
   "source": [
    "# Langchaing Conversation Model\n",
    "The following code allows us to use conversation models with langchain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3009f2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "    \n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d697cdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Templates\n",
    "customer_message = \"\"\" Me podrían indicar por favor cual es el horario \\\n",
    "    de atención de la IPS Cuidarte? Necesito sacar una cita \\\n",
    "    con medico general\n",
    "\"\"\"\n",
    "style = \"\"\" Colombian spanish \\\n",
    "    with respectful calm tone\n",
    "\"\"\"\n",
    "prompt = \"\"\" Translate the text \\\n",
    "    into a style that is {style}, \\\n",
    "    text: {text}\n",
    "\"\"\"\n",
    "prompt1 = \"\"\" From the given text, extract the following information: \\\n",
    "    company = The company of the request \\\n",
    "    type = The kind of appoitment \\\n",
    "    \n",
    "    Format the output as JSON with following the next keys:\n",
    "    company\n",
    "    type\n",
    "    text: {text}\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9be78c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f9a1fefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature = 0) # Temperature = 0 means less randomness\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm = llm,\n",
    "    memory = memory,\n",
    "    verbose = False, # !!! With this equls True, shows the prompt followed by chatGPT\n",
    ")\n",
    "\n",
    "# prompt_template = ChatPromptTemplate.from_template(prompt)\n",
    "\n",
    "# # Look for template inputs\n",
    "# prompt_template.messages[0].prompt.input_variables\n",
    "\n",
    "# #Assigning input variables\n",
    "# customer_messages = prompt_template.format_messages(style=style, text=customer_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cc88d3df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Julian! It's nice to meet you. How can I assist you today?\""
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    conversation.predict(input=\"My name is Julian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bdf188d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Julian.'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Whats my name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "247f8642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: My name is Julian\n",
      "AI: Hello Julian! It's nice to meet you. How can I assist you today?\n",
      "Human: Whats my name\n",
      "AI: Your name is Julian.\n"
     ]
    }
   ],
   "source": [
    "# This allows to see the memory of the chat\n",
    "print(memory.buffer)\n",
    "#memory.load_memory_variables({})\n",
    "#memory.savecontext({input:\"Hi\", output:\"What's up?\"}) # To manipulate the memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8eab0195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿Podrían por favor informarme cuál es el horario de atención de la IPS Cuidarte? Necesito agendar una cita con el médico general.\n"
     ]
    }
   ],
   "source": [
    "# !!!There is also another kind of memory\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=1) # k = 1 means the lag of the conversation\n",
    "\n",
    "#from langchain.memory import ConversationTokenBufferMemory # with tokens\n",
    "#memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=50)\n",
    "\n",
    "#from langchain.memory import ConversationSummaryBufferMemory # !!!with summary\n",
    "#memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=50)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
